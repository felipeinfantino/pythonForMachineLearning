{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before submitting\n",
    "1. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "2. Make sure that no assertions fail or exceptions occur, otherwise points will be subtracted.\n",
    "\n",
    "3. Please submit only the `*.ipynb` file.\n",
    "\n",
    "4. Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". Edit only between `YOUR CODE HERE` and `END YOUR CODE`.\n",
    "\n",
    "5. Make sure to use Python 3, not Python 2.\n",
    "\n",
    "Fill your group name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPNAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 1: Python Basics\n",
    "\n",
    "This first  exercise sheet tests the basic functionalities of the Python programming language in the context of a simple prediction task. We consider the problem of predicting health risk of subjects from personal data and habits. We first use for this task a decision tree\n",
    "\n",
    "![](tree.png)\n",
    "\n",
    "adapted from the webpage http://www.refactorthis.net/post/2013/04/10/Machine-Learning-tutorial-How-to-create-a-decision-tree-in-RapidMiner-using-the-Titanic-passenger-data-set.aspx. For this exercise sheet, you are required to use only pure Python, and to not import any module, including numpy. In exercise sheet 2, the nearest neighbor part of this exercise sheet will be revisited with numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a single instance (15 P)\n",
    "\n",
    "* Create a function that takes as input a tuple containing values for attributes (smoker,age,diet), and computes the output of the decision tree. Should return `\"less\"` or `\"more\"`.\n",
    "* Test your function on the tuple `('yes', 31, 'good')`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f3e47f689550d7323b8965c76a70298d",
     "grade": false,
     "grade_id": "cell-b66d7278bc313c94",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decision(x):\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    if len(x) != 3:\n",
    "        return 'wrong input'\n",
    "    if x[0] == 'yes':\n",
    "        if x[1] < 29.5:\n",
    "            return 'less'\n",
    "        else:\n",
    "            return 'more'\n",
    "    else:\n",
    "        if x[2] == 'good':\n",
    "            return 'less'\n",
    "        else:\n",
    "            return 'more'\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0559abdef335108b9edbb4ea3011c4c",
     "grade": true,
     "grade_id": "cell-c31b80471db3132f",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "x = ('yes', 31, 'good')\n",
    "assert decision(x) == 'more'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a dataset from a text file (10 P)\n",
    "\n",
    "The file `health-test.txt` contains several fictious records of personal data and habits.\n",
    "\n",
    "* Read the file automatically using the methods introduced during the lecture.\n",
    "* Represent the dataset as a list of tuples. Make sure that the tuples have the same format as above, e.g. `('yes', 31, 'good')`.\n",
    "* Make extra note of the datatype of each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8217159008caa1db2f6649e3ed092d9f",
     "grade": false,
     "grade_id": "cell-c1a8bc4c0e4ccb26",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gettest():\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    f = open(\"health-test.txt\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        #get rid of the '\\n' character\n",
    "        line = line[:-1]\n",
    "        #split the line \n",
    "        temp_tuple = line.split(\",\")\n",
    "        #cast the second element\n",
    "        temp_tuple[1] = float(temp_tuple[1])\n",
    "        #cast to tuple\n",
    "        temp_tuple = tuple(temp_tuple)\n",
    "        dataset.append(temp_tuple)\n",
    "    return dataset\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a6c609343f9b1d4bb9e02d4cc0abc2e",
     "grade": true,
     "grade_id": "cell-4e1f7ad1e66b3121",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the decision tree to the dataset (15 P)\n",
    "\n",
    "* Apply the decision tree to all points in the dataset, and return the ratio of them that are classified as \"more\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6a8ebad6d019805a1f317cd25c329cff",
     "grade": false,
     "grade_id": "cell-6703ef98e2b5c93b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_testset():\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    dataset = gettest()\n",
    "    total_of_tuples = len(dataset)\n",
    "    more_classified_tuples = 0\n",
    "    for i in dataset:\n",
    "        if decision(i) == 'more':\n",
    "            more_classified_tuples = more_classified_tuples +1\n",
    "    return more_classified_tuples / total_of_tuples\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cf754f1f4a98838c63a6840e15ae198",
     "grade": true,
     "grade_id": "cell-c13a0b23c9faba52",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from examples (10 P)\n",
    "\n",
    "Suppose that instead of relying on a fixed decision tree, we would like to use a data-driven approach where data points are classified based on a set of training observations manually labeled by experts. Such labeled dataset is available in the file `health-train.txt`. The first three columns have the same meaning than for `health-test.txt`, and the last column corresponds to the labels.\n",
    "\n",
    "* Write a procedure that reads this file and converts it into a list of pairs. The first element of each pair is a triplet of attributes, and the second element is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e6de29aa89d0457a5c0aeb5d7123a2ef",
     "grade": false,
     "grade_id": "cell-fc38ed11fee6fbeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gettrain():\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    f = open(\"health-train.txt\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        #get rid of the '\\n' character\n",
    "        line = line[:-1]\n",
    "        #split the line \n",
    "        temp_tuple = line.split(\",\")\n",
    "        #cast the second element\n",
    "        temp_tuple[1] = float(temp_tuple[1])\n",
    "        #Now we have a 4 elemental list, so we pop the last element and store it\n",
    "        label = temp_tuple.pop(3)\n",
    "        #cast to tuple\n",
    "        temp_tuple = tuple(temp_tuple)\n",
    "        #append the pair\n",
    "        dataset.append((temp_tuple, label))\n",
    "    return dataset\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "925716f474f7b0e3a7892e1aebfa217e",
     "grade": true,
     "grade_id": "cell-a3d593f232e0403a",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor classifier (25 P)\n",
    "\n",
    "We consider the nearest neighbor algorithm that classifies test points following the label of the nearest neighbor in the training data. For this, we need to define a distance function between data points. We define it to be\n",
    "\n",
    "`d(a, b) = (a[0] != b[0]) + ((a[1] - b[1]) / 50.0) ** 2 + (a[2] != b[2])`\n",
    "\n",
    "where `a` and `b` are two tuples corrsponding to the attributes of two data points.\n",
    "\n",
    "* Write a function that retrieves for a test point the nearest neighbor in the training set, and classifies the test point accordingly.\n",
    "* Test your function on the tuple `('yes', 31, 'good')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87751ef965d915307f761da8071a184f",
     "grade": false,
     "grade_id": "cell-671ea24ec8a11241",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def distance_neighbor(a,b):\n",
    "    return (a[0] != b[0]) + ((a[1] - b[1]) / 50.0) ** 2 + (a[2] != b[2])\n",
    "\n",
    "def neighbor(x, trainset):\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    #we define a tuple, that gets the distance between x and a point in trainset, and the tuple in the trainset\n",
    "    #we initalized it with infinity and None\n",
    "    min_distance_tuple = (float(\"inf\"),None)\n",
    "    #We iterate over all the trainset, and everytime we find a smaller distance we update our tuple\n",
    "    for i in trainset:\n",
    "        temp_distance = distance_neighbor(x, i[0])\n",
    "        if temp_distance < min_distance_tuple[0]:\n",
    "            min_distance_tuple = (temp_distance,i)\n",
    "    \n",
    "    #we just want the label from the tuple so\n",
    "    return min_distance_tuple[1][1]\n",
    "    # Beeing min_distance_tuple[1] a tuple in the format ('yes', 33.0, 'good'), 'more')\n",
    "    # And from that tuple we want the label so the [1] element\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "528146e447a25173480343d5fb7fd585",
     "grade": true,
     "grade_id": "cell-a36122337853f195",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "x = ('yes', 31, 'good')\n",
    "assert neighbor(x, gettrain()) == \"more\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply both the decision tree and nearest neighbor classifiers on the test set, and return the list of data point(s) for which the two classifiers disagree, and with which probability it happens.\n",
    "* A data point should look like above, e.g. `('yes', 31, 'good')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d26e5477527236f7db5c7cb6f8589c3a",
     "grade": false,
     "grade_id": "cell-8dbf7da153f3d797",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compare():\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    dataset = gettrain()\n",
    "    Xdisagree = []\n",
    "    for i in dataset:\n",
    "        if decision(i[0]) != neighbor(i[0], dataset):\n",
    "            Xdisagree.append(i[0])\n",
    "    probability = float(len(Xdisagree)/len(dataset))\n",
    "    # <<<<< END YOUR CODE\n",
    "    return Xdisagree, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa0b2ace7df5181b0528208ca0cc0dcc",
     "grade": true,
     "grade_id": "cell-3b55f7e89ad4dfeb",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yes', 54.0, 'good')]\n"
     ]
    }
   ],
   "source": [
    "Xdisagree, probability = compare()\n",
    "assert type(Xdisagree) == list\n",
    "print(Xdisagree)\n",
    "assert probability >= 0.0 and probability <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem of simple nearest neighbors is that one needs to compare the point to predict to all data points in the training set. This can be slow for datasets of thousands of points or more. Alternatively, some classifiers train a model first, and then use it to classify the data.\n",
    "\n",
    "## Nearest mean classifier (25 P)\n",
    "\n",
    "We consider one such trainable model, which operates in two steps:\n",
    "\n",
    "(1) Compute the average point for each class, (2) classify new points to be of the class whose average point is nearest to the point to predict.\n",
    "\n",
    "For this classifier, we convert the attributes smoker and diet to real values (for smoker: yes=1.0 and no=0.0, and for diet: good=0.0 and poor=1.0), and use the modified distance function:\n",
    "\n",
    "`d(a,b) = (a[0] - b[0]) ** 2 + ((a[1] - b[1]) / 50.0) ** 2 + (a[2] - b[2]) ** 2`\n",
    "\n",
    "We adopt an object-oriented approach for building this classifier.\n",
    "\n",
    "* Implement the methods `train` and `predict` of the class `NearestMeanClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "303bfe831913b6c12d012e06b0815e0c",
     "grade": false,
     "grade_id": "cell-e0b339bfd0fcc16c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def distance(a,b):\n",
    "    return (a[0] - b[0]) ** 2 + ((a[1] - b[1]) / 50.0) ** 2 + (a[2] - b[2]) ** 2\n",
    "\n",
    "#we have data in form from ((str,float,str),str) , we want ((float,float,float),str) in order to compute the mean\n",
    "def convert_data(dataset):\n",
    "    converted_data_set = []\n",
    "    for i in dataset:\n",
    "        converted_smoker = 1.0 if i[0][0] == \"yes\" else 0.0\n",
    "        converted_diet = 1.0 if i[0][2] == \"poor\" else 0.0\n",
    "        converted_tuple = (converted_smoker, i[0][1],converted_diet)\n",
    "        converted_data_set.append((converted_tuple,i[1]))\n",
    "    return converted_data_set\n",
    "\n",
    "class NearestMeanClassifier:\n",
    "    def train(self, dataset):\n",
    "        # >>>>> YOUR CODE HERE\n",
    "        dataset = convert_data(dataset)\n",
    "        self.mean_points_tuple_list = [] # A list of tuples. Each tuple is in format (mean_tuple,class), there is only one tuple of each class\n",
    "        # Then we have a dict, where the key is the class and the value is list of four elements as follows: [sum_smoker, sum_age, sum_diet, counter_of_elements]\n",
    "        # The idea is to iterate just once over all the dataset, and keep summing the features as well as counting them , so we could divied all the sums by the counted elements for each class\n",
    "        sum_dict = {}\n",
    "        # Im assuming that the dataset is a list of tuples in format ((smoker, age, diet),class)\n",
    "        for i in dataset:\n",
    "            label = i[1] # the class\n",
    "            list_of_features = [i[0][0], i[0][1], i[0][2], 1] # smoker, age, diet, counter\n",
    "            current_list = sum_dict.get(label) \n",
    "            #If the class is not in the dict we add it, otherwise we sum the items\n",
    "            if current_list is None:\n",
    "                sum_dict[label] = list_of_features\n",
    "            else:\n",
    "                # We found an element in the dict, so we increase the counter\n",
    "                current_list[3] = current_list[3] + 1\n",
    "                # We sum then all the features\n",
    "                sum_dict[label] = [x + y for x, y in zip(current_list, list_of_features)]\n",
    "        \n",
    "        # Now we iterate over the sum_dict, and append the mean tuples , with their class to the mean_points_tuple_list\n",
    "        for label, summed_list_of_features in sum_dict.items():\n",
    "            total_of_elements = summed_list_of_features[3]\n",
    "            \n",
    "            average_smoker_value = float(summed_list_of_features[0] / total_of_elements)\n",
    "            average_age_value = float(summed_list_of_features[1] / total_of_elements)\n",
    "            average_diet_value =  float(summed_list_of_features[2] / total_of_elements)\n",
    "\n",
    "            self.mean_points_tuple_list.append(((average_smoker_value,average_age_value, average_diet_value), label))\n",
    "        \n",
    "        # <<<<< END YOUR CODE\n",
    "\n",
    "    def predict(self, x):\n",
    "        # >>>>> YOUR CODE HERE\n",
    "        #x is in format (str,float,str) we need (float,float,float)\n",
    "        \n",
    "        smoker = 1.0 if x[0] == \"yes\" else 0.0\n",
    "        diet = 1.0 if x[2] == \"poor\" else 0.0\n",
    "        x = (smoker, x[1], diet) \n",
    "        \n",
    "        if not self.mean_points_tuple_list:\n",
    "            return \"You should train first, before predicting\"\n",
    "        #same logic as in the neighbor function\n",
    "        min_distance_tuple = (float(\"inf\"),None)\n",
    "        prediction = \"\"\n",
    "        #We iterate over all the trainset, and everytime we find a smaller distance we update our tuple\n",
    "        for i in self.mean_points_tuple_list:\n",
    "            temp_distance = distance(x, i[0])\n",
    "            if temp_distance < min_distance_tuple[0]:\n",
    "                min_distance_tuple = (temp_distance,i)\n",
    "    \n",
    "        #we just want the label from the tuple so\n",
    "        prediction = min_distance_tuple[1][1]\n",
    "        # <<<<< END YOUR CODE\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build an object of class `NearestMeanClassifier`, train it on the training data, and return the mean vector for each class. You should return a dictionary with keys `less` and `more`. Each key should correspond to a mean vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e41c5c9134feb3a505677bc939acc25d",
     "grade": false,
     "grade_id": "cell-5f7f00ee83c94703",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'less': (0.17647058823529413, 17.0, 0.11764705882352941),\n",
       " 'more': (0.3076923076923077, 20.0, 0.3076923076923077)}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasifier = NearestMeanClassifier()\n",
    "\n",
    "def build_and_train():\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    dataset = gettrain()\n",
    "    clasifier.train(dataset)\n",
    "    # We have the data store in the mean_points_tuple_list variable, so we just convert it to a dict and return it\n",
    "    returnable_dict = {}\n",
    "    for i in clasifier.mean_points_tuple_list:\n",
    "        returnable_dict[i[1]] = i[0]\n",
    "    return returnable_dict\n",
    "    # <<<<< END YOUR CODE\n",
    "build_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "686cebaaedb7e41b87361a8408661ab1",
     "grade": true,
     "grade_id": "cell-415891bde4cbde19",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predict the test data using the nearest mean classifier and return all test examples for which all three classifiers (decision tree, nearest neighbor and nearest mean) agree. You should return a list containing tuples. Each tuple should contain the datapoint and the prediction i.e.\n",
    "\n",
    "`[(('no', 50, 'good'), 'less'), ... ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "29945175006a71d97e57588c9c524df1",
     "grade": false,
     "grade_id": "cell-f37f3035a32a8f85",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('no', 23.0, 'good'), 'less'),\n",
       " (('no', 15.0, 'poor'), 'more'),\n",
       " (('no', 50.0, 'good'), 'less'),\n",
       " (('no', 18.0, 'good'), 'less'),\n",
       " (('no', 60.0, 'good'), 'less'),\n",
       " (('yes', 45.0, 'poor'), 'more')]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_test():\n",
    "    # >>>>> YOUR CODE HERE    \n",
    "    dataset = gettest()\n",
    "    dataset_for_training = gettrain()\n",
    "    \n",
    "    n_mean_samples = []\n",
    "    n_neighbor_samples = []\n",
    "    decision_tree_samples = []\n",
    "    \n",
    "    for i in dataset:\n",
    "        n_mean_samples.append((i,clasifier.predict(i)))\n",
    "        n_neighbor_samples.append((i, neighbor(i,dataset_for_training)))\n",
    "        decision_tree_samples.append((i, decision(i)))\n",
    "\n",
    "    \n",
    "    list_of_lists = []\n",
    "    list_of_lists.append(n_mean_samples)\n",
    "    list_of_lists.append(n_neighbor_samples)\n",
    "    list_of_lists.append(decision_tree_samples)\n",
    "    \n",
    "    agreed_samples = list(set(list_of_lists[0]).intersection(*list_of_lists))\n",
    "    \n",
    "    # <<<<< END YOUR CODE\n",
    "    return agreed_samples\n",
    "predict_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "951f7d822b4c3475ab6dc89fa3c87d46",
     "grade": true,
     "grade_id": "cell-853c957eaaf81c28",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
